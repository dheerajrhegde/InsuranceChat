{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c97dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.v1 import BaseModel, Field\n",
    "from langchain.agents import tool\n",
    "import requests, json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DoctorSearch(BaseModel):\n",
    "    zip_code: str = Field(..., description=\"Zip code in which to search for doctors\")\n",
    "        \n",
    "@tool(args_schema=DoctorSearch)\n",
    "def get_doctors(zip_code: str) -> list:\n",
    "    \"\"\"Get doctors for the given zip code\"\"\"\n",
    "    url = \"https://fhir.cigna.com/ProviderDirectory/v1/Location?address-postalcode=\" + zip_code\n",
    "    jsonString = requests.get(url)\n",
    "    addresses = []\n",
    "    data = json.loads(jsonString.content)\n",
    "    data = data['entry']\n",
    "    for entry in data:\n",
    "        address = entry['resource']['address']['line']\n",
    "        addresses = addresses + address\n",
    "\n",
    "    return addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84490c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f043d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.3.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from datetime import date\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"Information about a doctor visit.\"\"\"\n",
    "    hospital_name: str = Field(description=\"Hospital name\")\n",
    "    visit_date: date = Field(description=\"date of visit\")\n",
    "        \n",
    "extraction_function = [convert_pydantic_to_openai_function(Claim)]\n",
    "#extraction_model = model.bind(functions=extraction_function, function_call={\"name\": \"Information\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c639376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.image import UnstructuredImageLoader\n",
    "\n",
    "class FileClaim(BaseModel):\n",
    "    file_name: str = Field(..., description=\"File to get claim information from\")\n",
    "\n",
    "@tool(args_schema=FileClaim)\n",
    "def process_file(file_name: str) -> dict:\n",
    "    \"\"\"Upload file and extract text\"\"\"\n",
    "    with open(file_name) as f:\n",
    "        loader = UnstructuredImageLoader(file_name)\n",
    "        data = loader.load()\n",
    "        model = ChatOpenAI(temperature=0)\n",
    "        extraction_model = model.bind(functions=extraction_function, function_call={\"name\": \"Claim\"})\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        extraction_chain = prompt | extraction_model\n",
    "        response = extraction_chain.invoke({\"input\": data[0]})\n",
    "        return response\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e7a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_claim_history():\n",
    "    \"\"\"\n",
    "    Get past claims raised\n",
    "    \"\"\"\n",
    "    num_claims = np.random.randint(2, 6)\n",
    "    print(num_claims)\n",
    "    claims = []\n",
    "    for _ in range(num_claims):\n",
    "        claim_id = np.random.randint(1000000, 2000000)\n",
    "        claim_type = random.choice([\"Hospitalization\", \"Out Patient\", \"Pharmacy\"])\n",
    "        claim_status = random.choice([\"Received\", \"Processed - Approved\", \"Processed - Denied\"])\n",
    "        claim_amount = np.random.randint(200, 500)\n",
    "        claims.append((claim_id, claim_type, claim_status, claim_amount))\n",
    "    return claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed1cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5871ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self):\n",
    "        os.environ['PINECONE_API_KEY'] = \"d21b8a7f-aa94-484e-8ca7-48eb3d1f0b11\"\n",
    "        index_name = \"insuranceplans\"\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "        metadata_field_info = [\n",
    "            AttributeInfo(\n",
    "                name=\"filename\",\n",
    "                description=\"The file the chunk is from. Is also the plan name\",\n",
    "                type=\"string\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        document_content_description = \"Cigna Plan Information\"\n",
    "        model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "        retriever = SelfQueryRetriever.from_llm(\n",
    "            llm=model,\n",
    "            document_contents=document_content_description,\n",
    "            metadata_field_info=metadata_field_info,\n",
    "            vectorstore=vectorstore,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Prompt template\n",
    "        template = \"\"\"Answer the question based only on the following context, \n",
    "        which can include text and tables: {context}\n",
    "        use the Question: {query}\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        self.chain = RunnableParallel(\n",
    "            {\n",
    "                'context': lambda x: retriever.get_relevant_documents(x[\"query\"]),\n",
    "                'query': lambda x: x[\"query\"],\n",
    "                #'plan_name': lambda x: x[\"plan_name\"]\n",
    "            }) | prompt | model | StrOutputParser()\n",
    "\n",
    "    def get_chain(self):\n",
    "        return self.chain\n",
    "\n",
    "class PlanInformation(BaseModel):\n",
    "    question: str = Field(..., description=\"The query to answer on the given plan_name\")\n",
    "        \n",
    "@tool(args_schema=PlanInformation)\n",
    "def get_plan_information(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Will take the plan_name and the question from the user and answer the question using\n",
    "    RAG chain created on plan documents\n",
    "    \"\"\"\n",
    "    chain = RAG().get_chain()\n",
    "    print(type(chain))\n",
    "    return chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5700f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.3.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n",
      "/Applications/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "\n",
    "functions = [\n",
    "    format_tool_to_openai_function(f) for f in [\n",
    "        get_plan_information, get_doctors, get_claim_history, process_file\n",
    "    ]\n",
    "] + extraction_function\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "response = model.invoke(\"./EavRM2VU8AEIZsP.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb2020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"file_name\":\"./EavRM2VU8AEIZsP.jpeg\"}', 'name': 'process_file'}}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 250, 'total_tokens': 277}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-b842949a-7aa7-4a4a-b0b0-4fb3a38d816b-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f1ef5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def run_agent(user_input):\\n    intermediate_steps = []\\n    while True:\\n        result = agent_chain.invoke({\\n            \"query\": user_input, \\n            \"intermediate_steps\": intermediate_steps\\n        })\\n        if isinstance(result, AgentFinish):\\n            return result\\n        tool = {\\n            \"get_doctors\": get_doctors, \\n            \"get_plan_information\": get_plan_information,\\n        }[result.tool]\\n        observation = tool.run(result.tool_input)\\n        intermediate_steps.append((result, observation))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | chain\n",
    "\n",
    "\"\"\"def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"query\": user_input, \n",
    "            \"intermediate_steps\": intermediate_steps\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"get_doctors\": get_doctors, \n",
    "            \"get_plan_information\": get_plan_information,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26e1fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "tools = [get_doctors, get_plan_information, get_claim_history, process_file]\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7f016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bae854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eff399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5dde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ef91c0e",
   "metadata": {},
   "source": [
    "### Use langchain to read text from images and pydantic to extract the data we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dbd6124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `process_file` with `{'file_name': './EavRM2VU8AEIZsP.jpeg'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mcontent='' additional_kwargs={'function_call': {'arguments': '{\"hospital_name\":\"Alta Bates Emergency\",\"visit_date\":\"6/6/2019\"}', 'name': 'Claim'}} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 605, 'total_tokens': 624}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-208be274-516f-45b8-8e27-5105240284b9-0'\u001b[0m\u001b[32;1m\u001b[1;3mI have extracted the following information from the file:\n",
      "- Hospital Name: Alta Bates Emergency\n",
      "- Visit Date: 6/6/2019\n",
      "\n",
      "Is there anything else you would like to know or do with this information?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I have extracted the following information from the file:\n",
      "- Hospital Name: Alta Bates Emergency\n",
      "- Visit Date: 6/6/2019\n",
      "\n",
      "Is there anything else you would like to know or do with this information?\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"./EavRM2VU8AEIZsP.jpeg\"}, return_only_outputs=True)\n",
    "print(response.get('output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda79f40",
   "metadata": {},
   "source": [
    "### We have a pinecone vector database in the background. get_plan_information tool invokes uses the pinecone database to get relevent information from documents and then Passes to OpenAI to create response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aab3211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_plan_information` with `{'question': 'Tell me about Connect Silver 1500 Indiv Med Deductible'}`\n",
      "\n",
      "\n",
      "\u001b[0m<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "\u001b[33;1m\u001b[1;3mThe Connect Silver 1500 Indiv Med Deductible is a health plan offered by Cigna HealthCare of North Carolina, Inc. for the coverage period from January 1, 2024, to December 31, 2024. It is available for both individuals and families and is classified as an HMO (Health Maintenance Organization) plan. \n",
      "\n",
      "The Summary of Benefits and Coverage (SBC) document for this plan provides details on how costs for covered health care services are shared between the plan and the insured. However, information about the cost of the plan, referred to as the premium, is provided separately. \n",
      "\n",
      "For more detailed information about the coverage or to obtain a complete copy of the terms of coverage, you can call 1-866-494-2111 or visit the website at https://www.cigna.com/ifp-documents. Additionally, for general definitions of common insurance terms, you can view the Glossary at https://www.healthcare.gov/sbc-glossary/ or call the same number to request a copy.\u001b[0m\u001b[32;1m\u001b[1;3mHere is some information about the Connect Silver 1500 Indiv Med Deductible plan offered by Cigna HealthCare of North Carolina, Inc. It is an HMO plan available for individuals and families for the coverage period from January 1, 2024, to December 31, 2024. If you need more details or have specific questions, you can contact Cigna at 1-866-494-2111 or visit their website.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here is some information about the Connect Silver 1500 Indiv Med Deductible plan offered by Cigna HealthCare of North Carolina, Inc. It is an HMO plan available for individuals and families for the coverage period from January 1, 2024, to December 31, 2024. If you need more details or have specific questions, you can contact Cigna at 1-866-494-2111 or visit their website.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Tell me about Connect Silver 1500 Indiv Med Deductible\"}, return_only_outputs=True)\n",
    "print(response.get('output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fdfbd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_plan_information` with `{'question': 'What is the overall deductible Connect Silver 1500 Indiv Med Deductible'}`\n",
      "\n",
      "\n",
      "\u001b[0m<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "\u001b[33;1m\u001b[1;3mThe overall deductible for Connect Silver 1500 Indiv Med Deductible is $1,500.\u001b[0m\u001b[32;1m\u001b[1;3mThe overall deductible for Connect Silver 1500 Indiv Med Deductible is $1,500.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The overall deductible for Connect Silver 1500 Indiv Med Deductible is $1,500.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"What is the overall deductible Connect Silver 1500 Indiv Med Deductible\"}, return_only_outputs=True)\n",
    "print(response.get('output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e86c3e",
   "metadata": {},
   "source": [
    "### Uses Cigna Location API to get the address of doctors in the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d837b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_doctors` with `{'zip_code': '28027'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['10030 EDISON SQUARE DR NW # 100A', '5325 VINNING ST NW # 101', '8815 CHRISTENBURY PKWY # 30', '8230 POPLAR TENT RD # 103', '10030 EDISON SQUARE DR NW # 100-A', '10030 EDISON SQUARE DR NW # 100-A', '10030 EDISON SQUARE DR NW # 100A', '10030 EDISON SQUARE DR NW # 100-A', '10030 EDISON SQUARE DR NW # 100-A', '7752 GATEWAY LN NW # 100', '8815 CHRISTENBURY PKWY # 30', '5641 POPLAR TENT RD # 101', '10030 EDISON SQUARE DR NW # 100', '5325 VINNING ST NW # 101', '5310 LANGFORD AVE NW', '8815 CHRISTENBURY PKWY # 30', '280 CONCORD PKWY S', '9955 POPLAR TENT RD', '7752 GATEWAY LN NW # 100', '369 CONCORD PKWY N']\u001b[0m\u001b[32;1m\u001b[1;3mHere are some doctors in the 28027 area:\n",
      "1. 10030 EDISON SQUARE DR NW # 100A\n",
      "2. 5325 VINNING ST NW # 101\n",
      "3. 8815 CHRISTENBURY PKWY # 30\n",
      "4. 8230 POPLAR TENT RD # 103\n",
      "5. 10030 EDISON SQUARE DR NW # 100-A\n",
      "6. 7752 GATEWAY LN NW # 100\n",
      "7. 5641 POPLAR TENT RD # 101\n",
      "8. 5310 LANGFORD AVE NW\n",
      "9. 280 CONCORD PKWY S\n",
      "10. 9955 POPLAR TENT RD\n",
      "11. 369 CONCORD PKWY N\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here are some doctors in the 28027 area:\n",
      "1. 10030 EDISON SQUARE DR NW # 100A\n",
      "2. 5325 VINNING ST NW # 101\n",
      "3. 8815 CHRISTENBURY PKWY # 30\n",
      "4. 8230 POPLAR TENT RD # 103\n",
      "5. 10030 EDISON SQUARE DR NW # 100-A\n",
      "6. 7752 GATEWAY LN NW # 100\n",
      "7. 5641 POPLAR TENT RD # 101\n",
      "8. 5310 LANGFORD AVE NW\n",
      "9. 280 CONCORD PKWY S\n",
      "10. 9955 POPLAR TENT RD\n",
      "11. 369 CONCORD PKWY N\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Get doctors in 28027\"}, return_only_outputs=True)\n",
    "print(response.get('output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a32a7",
   "metadata": {},
   "source": [
    "### Uses a dummary tool (get_claim_history) that creates randown claim history for LLM to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "598ce103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_claim_history` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m4\n",
      "\u001b[38;5;200m\u001b[1;3m[(1759320, 'Out Patient', 'Processed - Denied', 410), (1805351, 'Out Patient', 'Processed - Approved', 439), (1867415, 'Out Patient', 'Processed - Denied', 309), (1695292, 'Pharmacy', 'Processed - Approved', 322)]\u001b[0m\u001b[32;1m\u001b[1;3mYou have raised the following claims this year:\n",
      "1. Claim ID: 1759320, Type: Out Patient, Status: Processed - Denied, Amount: $410\n",
      "2. Claim ID: 1805351, Type: Out Patient, Status: Processed - Approved, Amount: $439\n",
      "3. Claim ID: 1867415, Type: Out Patient, Status: Processed - Denied, Amount: $309\n",
      "4. Claim ID: 1695292, Type: Pharmacy, Status: Processed - Approved, Amount: $322\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You have raised the following claims this year:\n",
      "1. Claim ID: 1759320, Type: Out Patient, Status: Processed - Denied, Amount: $410\n",
      "2. Claim ID: 1805351, Type: Out Patient, Status: Processed - Approved, Amount: $439\n",
      "3. Claim ID: 1867415, Type: Out Patient, Status: Processed - Denied, Amount: $309\n",
      "4. Claim ID: 1695292, Type: Pharmacy, Status: Processed - Approved, Amount: $322\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"What claims have I raised this year?\"}, return_only_outputs=True)\n",
    "print(response.get('output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7fa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e0b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ba60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d018f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fb851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859673fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb349c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb3ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363219c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38617d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360cb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad7f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd1623d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b28a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29699e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31be1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1a476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aaef48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cc63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55d04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b51e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098b826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834dd029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662b640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
